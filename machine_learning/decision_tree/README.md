---
layout:     post                   
title:      决策树                
date:       2019-05-18           
categories: 总结
tags:                       
    - 总结
mathjax: true
---

### 一、引言

> **决策树**采用的是自顶向下的递归方法，其基本思想想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点，熵值为0；
>
> ID3、C4.5、CART

### 二、决策树

#### 2.1、决策树时什么

> 决策树呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程；
>
> **学习时：**利用训练数据，根据损失函数最小化原则建立决策树模型；
>
> **预测时：**对新的数据，利用决策墨西哥进行分类；

决策树的分类：根据**目标变量**的类型，可以分类两类：

- **离散型决策树：**目标变量是离散的，如性别；
- **连续型决策树：**目标变量是连续的，如工资、价格；

**相关概念：**

- **根节点：**表示整个样本集合，并可以进一步划分成两个或多个子集合；
- **拆分：**表示将一个节点拆分成多个子节点的过程；
- **决策节点：**当一个节点被拆分成多个子节点时，当前结点被称为决策节点；
- **叶子结点：**无法继续拆分的节点；
- **剪枝：**移除决策树中某个节点的过程，与拆分过程相反；
- **分支/子树：**决策树的某一部分；
- **父节点/子节点：**一个节点被拆分成多个子节点；则该节点称为子接待你的父节点，其拆分的节点称为该节点的子节点；

![](https://res.cloudinary.com/chenzhen/image/upload/v1553220491/github_image/2019-03-22/1.png)

#### 2.2、决策树的构造过程

> 三个步骤：**特征选择、决策树生成、决策树剪枝；**

**（1）特征选择**

> 从训练数据的特征中选择一个特征作为当前节点的分裂标准；
>
> 特征选择的标准不同产生了不同的决策树算法：ID3（信息增益）、C4.5（信息增益比）、CART（Gini指数）等；

**准则：**使用某一个特征对数据集进行拆分，各数据子集的纯度要比划分前的数据纯度高；

**（2）决策树的生成**

根据特征选择的评估标准，从上至下的递归生成子节点，直到数据集不可再拆分；对与当前数据集的划分，使用某一特征划分后，各子集的纯度要高于拆分前的数据集的纯度，即不确定性要小；

**（3）决策树的剪枝**

决策树容易过拟合，利用剪枝（即删除某些节点）来缩小决策树的规模，缓解过拟合；

#### 2.3、决策树的优缺点

**（1）优点：**

- 可读性，即给定一个模型，可以根据决策树推理出相应的逻辑表达；
- 分类速度快；
- 对中间值的缺失不敏感；
- 可以处理不相关特征数据；

**（2）缺点：**

- 对未知的数据未必有好的分类、泛化能力；

### 三、ID3、C4.5、CART算法原理与代码实现

#### 3.1 ID3算法原理与代码实现

##### 3.1.1 信息增益

**（1）熵**

> 在信息论中，熵（Entropy）是随机变量不确定性的度量；即熵越大，则随机变量的不确定性就越高；

设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i) = p_i \qquad i = 1, 2, \cdots, n 
$$
则随机变量$X$的熵定义为：
$$
H(X) = -\sum_{i=1}^{n}p_i \log p_i
$$
**（2）条件熵**

> **条件熵：**表示已知随机变量$X$的条件下，随机变量$Y$的不确定性，即$H(Y|X)$；

随机变量$(X, Y)$的联合概率分布为：
$$
P(X=x_i, Y=y_i) = p_{ij} \qquad i =1, 2, \cdots, n \quad j = 1,2,\cdots, n
$$
则在随机变量$X$的给定的条件下，随机变量$Y$的条件概率分布的熵对$X$的数学期望：
$$
H(Y|X) = \sum_{i=1}^{n}p_i H(Y|X=x_i) \qquad p_i = P(X=x_i), \quad i = 1,2, \cdots, n
$$
**注意：**当熵和条件熵中的概率由数据估计得到时，对应的熵和条件熵被称为经验熵和经验条件熵；

**（3）信息增益**

**定义：**表示得知特征A的信息后，原始的数据集D的分类不确定性减少的程度：
$$
Gain(D, A) = H(D) - H(D|A)
$$
即集合D的经验熵$D(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差；

**即选择划分后信息增益大的特征作为划分特征；说明该使用该特征划分后，得到的子集纯度较高，即不确定性较小；**

##### 3.1.2 ID3算法步骤

输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$；

输出：决策树T；

**Step 1：**若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$作为该节点的类标记，返回T；

**Step 2：**若$A=\emptyset $，则T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T；

**Step 3：**否则，计算特征集A中所有特征的信息增益，选择信息增益最大的特征$A_k$作为划分特征；

**Step 4：**如果$A_k$的信息增益小于$\epsilon$，则T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T；

**Step 5：**否则，利用$A_k$的每一种取值$a_i$，将数据集D划分成若干个子集$D_i$，并将每一个子集$D_i$中实例数最大的类坐标类标记，构建子节点，有子节点及其子树构成树T，返回T；

**Step 6：**对第$i$个子节点，以$D_i$为训练集，以$A-A_k$为特征集合，递归调用**Step1~Step 5**，得到子树$T_i$，返回$T_i$；

##### 3.1.3 ID3算法代码实现

基于sklearn实现ID3

```python
"""
使用sklearn学习决策树Decision Tree
用于分类
"""
# ===============================================================
from sklearn import tree
from sklearn import datasets
from sklearn.externals.six import StringIO
import os
import pydot

# load datasets
iris = datasets.load_iris()
iris_data = iris.data
iris_label = iris.target

# Training
model_tree = tree.DecisionTreeClassifier(criterion="entropy").fit(iris_data, iris_label)

# predict
pred = model_tree.predict(iris_data)
acc = sum(pred == iris_label) / len(iris_label)
print(acc)

# export Decision Tree pdf
dot_data = StringIO()
tree.export_graphviz(model_tree, out_file=dot_data)
graph = pydot.graph_from_dot_data(dot_data.getvalue())
graph[0].write_pdf("iris.pdf")
```

![](https://res.cloudinary.com/chenzhen/image/upload/v1553233589/github_image/2019-03-22/iris.png)

**如果上述示例不直观，可以使用下述数据集：**

| 序号 | 不浮出水面是否可以生存 | 是否有脚蹼 | 是否属于鱼类 |
| :--: | :--------------------: | :--------: | :----------: |
|  1   |           是           |     是     |      是      |
|  2   |           是           |     是     |      是      |
|  3   |           是           |     否     |      否      |
|  4   |           否           |     是     |      否      |
|  5   |           否           |     是     |      否      |

```python
# 其他不需要改变，只需要改变训练数据以及对应标签
data_x = [[1, 1],
          [1, 1],
          [1, 0],
          [0, 1],
          [0, 1]]
data_y = [1, 1, 0, 0, 0]
# 再替换掉下面相应的代码即可；
```

![](https://res.cloudinary.com/chenzhen/image/upload/v1553234895/github_image/2019-03-22/data_self_tree.png)

#### 3.2 C.5算法原理与代码实现

##### 3.2.1 信息增益比

信息增益会偏向于取值较多的特征，使用**信息增益比**可以对这一问题进行校正；

**定义：**特征A对数据集D的信息增益$GainRatio(D,A)$定义为其信息增益与训练集D的经验熵$H(D)$的比值：
$$
GainRatio(D,A)=\dfrac{Gain(D,A)}{H(D}
$$

##### 3.2.2 C4.5算法

C4.5算法与ID3算法流程一致，只是将信息增益换成了信息增益比；

##### 3.2.3 代码实现



#### 3.3 CART算法原理与代码实现

##### 3.3.1 Gini指数

在分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：
$$
Gini(p) = \sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$
对于给定的样本集合D，其基尼指数为：
$$
Gini(D) = 1-\sum_{k=1}^{K}(\dfrac{|C_k|}{|D|})^2
$$
其中，$C_k$表示数据集D中第k类的样本子集，K表示类的个数；

**在特征A的条件下，集合D的基尼指数为：**
$$
Gini(D,A) = \dfrac{|D_1|}{|D|}Gini(D_1) + \dfrac{|D_2|}{|D|}Gini(D_2)
$$
基尼指数表示集合的不确定性，值越大，则集合的不确定性就越高；

##### 3.3.2 CART算法

输入：训练数据集D，特征集A，停止计算的条件；

输出：CART决策树

**Step 1：**首先，利用每一个特征的取值将数据集划分成不同子集，计算在特征$A_k$的条件下的基尼系数$Gini(D, A)$；

**Step 2：**得到所有可能的特征A以及对应所有可能的切分点，选择**基尼系数最小**的特征以及对应的切分点作为最优特征与最优却分点。并根据最优切分点，将数据集D划分成若干子集，并将训练集依次分配到子节点中；

**Step 3：**对两个子节点递归地调用**Step 1、Step 2**，直至满足条件；

**Step 4：**生成CART决策树；

算法停止计算的条件是：**节点中的样本个数小于预定阈值，或者样本集的基尼系数小于预定阈值，或者样本集中没有更多特征；**

##### 3.3.3 代码实现

> 可以将ID3算法的sklearn的实现，改动一处：

```python
model_tree = tree.DecisionTreeClassifier(criterion="gini").fit(iris_data, iris_label)
```



#### Reference：

- https://shuwoom.com/?p=1452
- https://www.cnblogs.com/coder2012/p/4508602.html
- https://blog.csdn.net/jiaoyangwm/article/details/79525237