## ResNet

* [Deep Residual Learning for Image Recognition](<https://arxiv.org/abs/1512.03385>)
* [残差网络ResNet笔记](https://www.cnblogs.com/alanma/p/6877166.html)
* [CNN网络架构演进：从LeNet到DenseNet](https://www.cnblogs.com/skyfsm/p/8451834.html)

---

**特点：**

* 网络层数超过百层；
* 引入残差块，解决退化问题；

---

> **泛逼近定理（Universal Approximation theorem）**
>
> 只要给定足够的容量，单层的前馈网络也足以表示任何函数。但是，该层可能非常庞大，网络核数据易出现过拟合。因此，研究界普遍认为网络架构需要更多层；

**网络加深**

&emsp; 然而，简单的增加网络的深度，会导致梯度弥散或梯度爆炸，对于该问题的解决方法是**正则化初始化**核**中间的正则化层（Batch Normalization）**，这样的话可以训练几十层的网络；

**退化问题：**

&emsp; 当网络到达一定的深度，其训练集上的准确率将会趋近饱和甚至下降，这就是退化问题；

&emsp; 作者通过实验：通过浅层网络+y=x等同于映射构造深层网络，结果深层网络模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是**因为深层网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数**；

**解决退化问题：**

&emsp; **深度残差网络**；**如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。**那么现在要解决的就是如何学习**恒等映射函数**。

&emsp; 如果直接让一些层去拟合一个潜在的恒等映射函数$H(x) = x​$是比较困难的，这可能就是深层网络难以训练的原因。但是如果将网络设计为$H(x) = F(x) + x​$，如下图。我们可以转换为**学习一个残差函数$F(x) = H(x)-x​$.只要$F(x) =0​$，就构成了一个恒等映射$H(x)  = x​$. **而且，拟合残差更加容易。

![](https://res.cloudinary.com/chenzhen/image/upload/v1558436789/github_image/2019-05-21/resnet_1.png)

---

---

另一种解释：

&emsp; 假设我们现在要训练一个深层的网络M，假设我们存在一个最优的网络N，与我们设计的这个网络M相比，我们设计的网络M必定会存在一些**多余的层**，那么，如果网络M中**这些多余的层的训练目标就是恒等变换**，恒等变换的意思就是这些多余的层不会对已经得到的结果造成任何改变；因此，这又达到这个目标（多余的层恒等变换），我们设计的网络的性能才能达到最优网络N一样；

那么，如何**对这些多余的层执行恒等变换呢？**

**传统网络中**

假设该层的输入为$x$，该层的输出是$F(x)$，而我们的输出目标则是$H(x)$；

因此，我们训练的目标是：$F(x)$与$H(x)$相等，即$F(x) = H(x)$；

**残差网络中**

同样，假设该层的输入为$x$，该层的输出为$F(x)+x$，我们的输出目标是$H(x)$；

因此，我们训练的目标是：$F(x)+x$与$H(x)$相等，即$F(x) + x = H(x)$，即$F(x)= H(x)-x$；



对多余的层执行恒等变换的话，我们对于该层的输出目标$H(x)$的值等于$x$，即$H(x)=x$；

那么，在传统网络中，如何对多余的层执行恒等变换呢？即目标是该层输出$F(x)$的值与$x$相等，即$F(x)=x$，这比较困难（至于怎么解释，不太明白，可能是之前都是这样做的，效果却不太好吧。）；

而在残差网络中，如何对多余的层执行恒等变换呢？同样地，即目标是该层输出$F(x)+x$的值与$x$相等，即$F(x)+x=x$，即要求$F(x)=0$；具体$F(x)=0$是什么意思呢？表示在上图中，**就是中间两个层啥用都没有，结果$F(x)=0$**；

---

---

下图展示了两种形态的残差模块，左图是常规残差模块，有两个$3\times 3$的卷积组成，但是随着网络进一步加深，这种残差结构在实践中并不是十分有效。针对这个问题，右图的“瓶颈残差模块”（bottleneck residual block）可以有更好的效果，它一次有$1\times 1, 3\times 3, 1\times 1$这三个卷积堆积而成，这里的$1\times 1$的卷积能够起到降维或升维的作用，而$3\times 3​$的卷积可以在相对较低维度的输入上进行，已达到提高计算效率的目的；

![](https://res.cloudinary.com/chenzhen/image/upload/v1558439580/github_image/2019-05-21/resnet_block.png)

**网络结构：**

![](https://res.cloudinary.com/chenzhen/image/upload/v1558440350/github_image/2019-05-21/resnet_architectures.png)

